import torch
import cv2
import timm
import os
import numpy as np
import re
import matplotlib.pyplot as plt
from ultralytics import YOLO
from torchvision import transforms
from PIL import Image
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from torch import nn

# âœ… ëª¨ë¸ ê²½ë¡œ ì„¤ì •
yolo_model_path = r"C:\Users\n3225\OneDrive\Desktop\model_test\best.pt"
hybrid_model_path = r"C:\Users\n3225\pycharm_source_code\python\imageprocessing\.venv\Scripts\capstone\hybrid_fish_classifier.pth"
image_folder = r"C:\Users\n3225\OneDrive\Desktop\model_test\fishing_test_dataset"

# âœ… YOLOv8 ëª¨ë¸ ë¡œë“œ (ë¬¼ê³ ê¸° íƒì§€)
yolo_model = YOLO(yolo_model_path)

# âœ… HybridFishClassifier ì •ì˜ (CNN + DeiT)
class HybridFishClassifier(nn.Module):
    def __init__(self, num_classes=13):
        super().__init__()

        # CNN ë°±ë³¸ (EfficientNet)
        self.cnn = timm.create_model('efficientnet_b0', pretrained=False, num_classes=0)
        self.cnn_out_dim = self.cnn.num_features  # CNN ì¶œë ¥ íŠ¹ì§• ì°¨ì›

        # DeiT ë°±ë³¸
        self.transformer = timm.create_model('deit_small_patch16_224', pretrained=False, num_classes=0)
        self.transformer_out_dim = 384  # DeiT-Smallì˜ ì„ë² ë”© ì°¨ì›

        # ìµœì¢… ë¶„ë¥˜ê¸° (CNN + Transformer íŠ¹ì§• ê²°í•©)
        self.fc = nn.Linear(self.cnn_out_dim + self.transformer_out_dim, num_classes)

    def forward(self, x):
        cnn_feat = self.cnn(x)  # CNN íŠ¹ì§• ì¶”ì¶œ
        trans_feat = self.transformer(x)  # Transformer íŠ¹ì§• ì¶”ì¶œ
        combined = torch.cat((cnn_feat, trans_feat), dim=1)  # íŠ¹ì§• ê²°í•©
        return self.fc(combined)  # ìµœì¢… ë¶„ë¥˜

# âœ… Hybrid ëª¨ë¸ ë¡œë“œ
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
hybrid_model = HybridFishClassifier(num_classes=5)
hybrid_model.load_state_dict(torch.load(hybrid_model_path, map_location=device))
hybrid_model.to(device)
hybrid_model.eval()

# âœ… í´ë˜ìŠ¤ ë¦¬ìŠ¤íŠ¸ (5ê°œ ë¬¼ê³ ê¸° ì¢…)
class_names = [
    "gamseongdom",
    "jeomnongeo",
    "neobchinongeo", "nongeo", "saenunchi"
]
class_to_idx = {cls: idx for idx, cls in enumerate(class_names)}
idx_to_class = {v: k for k, v in class_to_idx.items()}

# âœ… ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (Hybrid ëª¨ë¸ ì…ë ¥ìš©)
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# âœ… ì„±ëŠ¥ í‰ê°€ë¥¼ ìœ„í•œ ë¦¬ìŠ¤íŠ¸
y_true = []
y_pred = []
detection_results = []  # ì´ë¯¸ì§€ ì‹œê°í™”ë¥¼ ìœ„í•´ ê²°ê³¼ ì €ì¥

# âœ… ì´ë¯¸ì§€ ì²˜ë¦¬ ë° ë¶„ë¥˜ í•¨ìˆ˜ (ê²°ê³¼ ì €ì¥)
def detect_and_classify(image_path):
    # ğŸ”¹ 1. ì´ë¯¸ì§€ ë¡œë“œ
    img = cv2.imread(image_path)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # ğŸ”¹ 2. YOLOv8ì„ ì‚¬ìš©í•˜ì—¬ ë¬¼ê³ ê¸° íƒì§€
    results = yolo_model(img_rgb)

    if len(results[0].boxes) == 0:
        print(f"âŒ {os.path.basename(image_path)}: ë¬¼ê³ ê¸° íƒì§€ ì‹¤íŒ¨")
        return None, None, None

    # ğŸ”¹ 3. ê°€ì¥ í™•ë¥ ì´ ë†’ì€ ë°”ìš´ë”© ë°•ìŠ¤ ì„ íƒ
    best_box = max(results[0].boxes, key=lambda b: b.conf)
    x1, y1, x2, y2 = map(int, best_box.xyxy[0].tolist())  # ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ

    # ğŸ”¹ 4. ì´ë¯¸ì§€ í¬ë¡­
    cropped_fish = img_rgb[y1:y2, x1:x2]
    cropped_fish_pil = Image.fromarray(cropped_fish)

    # ğŸ”¹ 5. Hybrid ëª¨ë¸ë¡œ ë¬¼ê³ ê¸° ì¢… ë¶„ë¥˜
    input_tensor = transform(cropped_fish_pil).unsqueeze(0).to(device)

    with torch.no_grad():
        outputs = hybrid_model(input_tensor)
        _, pred_idx = torch.max(outputs, 1)

    predicted_class = class_names[pred_idx.item()]

    # ğŸ”¹ 6. ì •ë‹µ ë ˆì´ë¸” ì¶”ì¶œ (íŒŒì¼ëª… ê¸°ë°˜)
    true_label = "Unknown"
    match = re.match(r"(.+?)\s*\(\d+\)", os.path.basename(image_path))
    if match:
        true_label = match.group(1)
        if true_label in class_to_idx:
            true_idx = class_to_idx[true_label]  # ì •ë‹µ ì¸ë±ìŠ¤ ë³€í™˜
            y_true.append(true_idx)
            y_pred.append(pred_idx.item())

    # ğŸ”¹ 7. ê²°ê³¼ ì €ì¥ (ì‹œê°í™”ë¥¼ ìœ„í•´)
    detection_results.append((img_rgb, cropped_fish_pil, true_label, predicted_class, (x1, y1, x2, y2), image_path))

# âœ… ì´ë¯¸ì§€ í´ë” ë‚´ ëª¨ë“  ì´ë¯¸ì§€ ì²˜ë¦¬ (ì˜ˆì¸¡ ìˆ˜í–‰)
for file in os.listdir(image_folder):
    if file.endswith(('.jpg', '.png', '.jpeg')):
        image_path = os.path.join(image_folder, file)
        detect_and_classify(image_path)

# âœ… ì„±ëŠ¥ ì§€í‘œ ì¶œë ¥ (ì˜ˆì¸¡ í›„)
if y_true and y_pred:
    print("\nğŸ¯ **ì„±ëŠ¥ í‰ê°€ ê²°ê³¼** ğŸ¯")
    accuracy = accuracy_score(y_true, y_pred)
    print(f"âœ… ì •í™•ë„: {accuracy * 100:.2f}%")
    print("\nğŸ“Š **ë¶„ë¥˜ ë³´ê³ ì„œ** ğŸ“Š")
    print(classification_report(y_true, y_pred, target_names=class_names))
    print("\nğŸ”¹ **í˜¼ë™ í–‰ë ¬** ğŸ”¹")
    print(confusion_matrix(y_true, y_pred))

# âœ… ì‹œê°í™” í•¨ìˆ˜ (íƒì§€ëœ ì´ë¯¸ì§€ ì¶œë ¥)
def visualize_detection(image, cropped_fish, true_label, predicted_class, bbox, image_path):
    """íƒì§€ëœ ë¬¼ê³ ê¸°ì™€ ë¶„ë¥˜ ê²°ê³¼ë¥¼ ì‹œê°í™”"""
    x1, y1, x2, y2 = bbox
    is_correct = true_label == predicted_class
    text_color = (0, 255, 0) if is_correct else (255, 0, 0)  # âœ… ì •ë‹µ: ì´ˆë¡ / ì˜¤ë‹µ: ë¹¨ê°•

    # ì›ë³¸ ì´ë¯¸ì§€ì— ë°”ìš´ë”© ë°•ìŠ¤ ë° ì •ë‹µ/ì˜ˆì¸¡ í´ë˜ìŠ¤ëª… í‘œì‹œ
    image_with_bbox = image.copy()
    cv2.rectangle(image_with_bbox, (x1, y1), (x2, y2), text_color, 2)
    cv2.putText(image_with_bbox, f"GT: {true_label}", (x1, y1 - 20),
                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)
    cv2.putText(image_with_bbox, f"Pred: {predicted_class}", (x1, y1 - 40),
                cv2.FONT_HERSHEY_SIMPLEX, 0.8, text_color, 2)

    # ì‹œê°í™”
    fig, axes = plt.subplots(1, 2, figsize=(10, 5))

    # ì›ë³¸ ì´ë¯¸ì§€ ì¶œë ¥
    axes[0].imshow(cv2.cvtColor(image_with_bbox, cv2.COLOR_BGR2RGB))
    axes[0].set_title("Detected Fish")
    axes[0].axis("off")

    # í¬ë¡­ëœ ì´ë¯¸ì§€ ì¶œë ¥
    if cropped_fish is not None:
        axes[1].imshow(cropped_fish)
        axes[1].set_title(f"GT: {true_label}\nPred: {predicted_class}", color=("green" if is_correct else "red"))
        axes[1].axis("off")

    # ê²°ê³¼ ì €ì¥
    result_dir = os.path.join(os.path.dirname(image_path), "results")
    os.makedirs(result_dir, exist_ok=True)
    plt.savefig(os.path.join(result_dir, os.path.basename(image_path)))
    plt.show()

# âœ… ê°œë³„ ì´ë¯¸ì§€ ê²°ê³¼ ì‹œê°í™”
for result in detection_results:
    visualize_detection(*result)
